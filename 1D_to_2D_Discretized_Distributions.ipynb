{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63ff18b-7ba4-41e3-b517-17e21b2ee0a3",
   "metadata": {},
   "source": [
    "# ðŸŒ³ 1D to 2D Classification using Decision Trees\n",
    "\n",
    "This notebook demonstrates how to classify data by learning the mapping from 1D measurements (e.g., chord lengths) to 2D shape categories using decision tree algorithms. Such tasks arise in stereology and materials analysis where observed features are lower-dimensional projections of higher-dimensional structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940fa79-8683-4741-be5e-4cef2531a97f",
   "metadata": {},
   "source": [
    "## Inferring Sphere Radius from Chord Length Distributions\n",
    "\n",
    "This script simulates 2D cuts through randomly packed spheres and collects the resulting chord lengths. These are converted into binned histograms (feature vectors) used to identify the original sphere radius via pattern matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88298b47-fb9e-406b-9012-e90ab1659164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def generate_circles(num_circles, radius):\n",
    "    positions = []\n",
    "    for _ in range(num_circles):\n",
    "        while True:\n",
    "            x = np.random.uniform(radius, 1 - radius)\n",
    "            y = np.random.uniform(radius, 1 - radius)\n",
    "            if all(np.sqrt((x - px) ** 2 + (y - py) ** 2) >= 2 * radius for px, py in positions):\n",
    "                positions.append((x, y))\n",
    "                break\n",
    "    return positions\n",
    "\n",
    "def chord_length(radius, d_line):\n",
    "    return 2 * np.sqrt(radius**2 - d_line**2)\n",
    "\n",
    "def distance_from_line(a, b, x, y):\n",
    "    return abs(a * x - y + b) / np.sqrt(a**2 + 1)\n",
    "\n",
    "def collect_chords_for_cuts(positions, radius, num_cuts, y_range=(0, 1)):\n",
    "    chord_lengths_per_cut = []\n",
    "    # Spacing for horizontal cuts\n",
    "    y_cut_values = np.linspace(y_range[0], y_range[1], num_cuts) \n",
    "    # Collect chord lengths for each cut\n",
    "    for y_cut in y_cut_values:\n",
    "        cut_chords = []\n",
    "        for (x, y) in positions:\n",
    "            # Calculate the perpendicular distance from the sphere center to the cut\n",
    "            d_line = abs(y - y_cut)  # This is simply the vertical distance since it's a horizontal line\n",
    "\n",
    "            # Check if the line intersects the sphere (distance must be less than the radius)\n",
    "            if d_line < radius:\n",
    "                length = chord_length(radius, d_line)\n",
    "                cut_chords.append(length)      \n",
    "        # Sort the chords for the current cut\n",
    "        chord_lengths_per_cut.append(sorted(cut_chords))\n",
    "    \n",
    "    return chord_lengths_per_cut\n",
    "\n",
    "\n",
    "def discretize_chord_lengths(chord_lengths, num_bins, range_min = 0, range_max=0.16):\n",
    "    bin_edges = np.linspace(range_min, range_max, num_bins + 1)\n",
    "    \n",
    "    # Initialize a list to store the count of chord lengths in each bin\n",
    "    bin_counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    # For each chord length, find which bin it belongs to and increment the count for that bin\n",
    "    for chord in chord_lengths:\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] < chord <= bin_edges[i + 1]:\n",
    "                bin_counts[i] += 1\n",
    "                break\n",
    "\n",
    "    return bin_counts\n",
    "    \n",
    "\n",
    "def distribution_chord_lengths(chord_lengths_per_cut, num_bins):\n",
    "    discretized_vectors = []\n",
    "    \n",
    "    for chord_lengths in chord_lengths_per_cut:\n",
    "        if chord_lengths:  # If it's not empty, discretize\n",
    "            discretized_vector = discretize_chord_lengths(chord_lengths, num_bins)\n",
    "        #else:  # If it's empty, create a zeroed vector of the same length as the bins\n",
    "        #    discretized_vector = np.zeros(num_bins, dtype=int)\n",
    "        #\n",
    "            discretized_vectors.append(discretized_vector)\n",
    "    \n",
    "    return discretized_vectors\n",
    "\n",
    "\n",
    "def flatten_discretized_vectors(discretized_vectors):\n",
    "    # Flatten each discretized vector into a 1D feature vector\n",
    "    return np.array([vec.flatten() for vec in discretized_vectors])\n",
    "\n",
    "def create_feature_radius_pairs(distributions, true_radii):\n",
    "    # Ensure the number of distributions matches the number of true radii\n",
    "    if len(distributions) != len(true_radii):\n",
    "        raise ValueError(\"The number of distributions must match the number of true radii.\")\n",
    "    \n",
    "    # Flatten the distributions\n",
    "    flattened_distributions = [flatten_discretized_vectors(distribution) for distribution in distributions]\n",
    "    \n",
    "    # Pair each flattened vector with its corresponding true radius\n",
    "    feature_radius_pairs = []\n",
    "    for i, flattened_distribution in enumerate(flattened_distributions):\n",
    "        for feature_vector in flattened_distribution:\n",
    "            feature_radius_pairs.append((feature_vector, true_radii[i]))\n",
    "    \n",
    "    return feature_radius_pairs\n",
    "\n",
    "def compute_vector_frequencies(distribution):\n",
    "    vector_counts = defaultdict(int)\n",
    "\n",
    "    for vector in distribution:\n",
    "        vector_tuple = tuple(vector)  # Convert numpy array to tuple for hashing\n",
    "        vector_counts[vector_tuple] += 1\n",
    "\n",
    "    return vector_counts\n",
    "\n",
    "\n",
    "def discretize_vector(vector, num_bins=10, range_min=0, range_max=0.16):\n",
    "    # Discretize the observed vector using the same bins as the training data\n",
    "    bin_edges = np.linspace(range_min, range_max, num_bins + 1)\n",
    "    discretized_vector = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    for val in vector:\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] < val <= bin_edges[i + 1]:\n",
    "                discretized_vector[i] += 1\n",
    "                break\n",
    "\n",
    "    return tuple(discretized_vector)  # Convert to tuple for comparison in dictionary\n",
    "\n",
    "# Function to find the best match across multiple dictionaries\n",
    "def find_best_match_in_dictionaries(observed_vector, dictionaries, num_bins=10, range_min=0, range_max=0.16):\n",
    "    # Discretize the observed vector\n",
    "    discretized_observed = discretize_vector(observed_vector, num_bins, range_min, range_max)\n",
    "    #discretized_observed = (np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0))\n",
    "    best_match = None\n",
    "    best_frequency = 0\n",
    "    best_dict_name = None\n",
    "\n",
    "    # Loop through each dictionary and check for matches\n",
    "    for dict_name, vector_frequencies in dictionaries.items():\n",
    "        # Find matches in the current dictionary\n",
    "        matching_frequencies = {vec: count for vec, count in vector_frequencies.items() if vec == discretized_observed}\n",
    "        \n",
    "        if matching_frequencies:\n",
    "            # Find the highest frequency in this dictionary\n",
    "            max_frequency_in_dict = max(matching_frequencies.values())\n",
    "            \n",
    "            if max_frequency_in_dict > best_frequency:\n",
    "                best_frequency = max_frequency_in_dict\n",
    "                best_match = discretized_observed\n",
    "                best_dict_name = dict_name\n",
    "\n",
    "    return best_match, best_frequency, best_dict_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff11d38-567e-490e-a97b-5cccdcf52384",
   "metadata": {},
   "source": [
    "## Simulating Chord Distributions for Different Sphere Densities\n",
    "\n",
    "We simulate 2D cross-sections through sets of non-overlapping spheres (with fixed radius) at four different densities. For each case, we compute chord lengths from horizontal cuts and discretize them into histograms for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75d26d6d-0ece-46a0-9a8d-909f581de36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_cuts = 1000  # Number of horizontal cuts\n",
    "num_bins = 20 #feature vectors\n",
    "\n",
    "\n",
    "radius = 0.08  # Radius of each circle\n",
    "\n",
    "positions = generate_circles(17, radius)\n",
    "chord_lengths_set_17 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_17 = distribution_chord_lengths(chord_lengths_set_17, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_circles(15, radius)\n",
    "chord_lengths_set_15 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_15 = distribution_chord_lengths(chord_lengths_set_15, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_circles(13, radius)\n",
    "chord_lengths_set_13 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_13 = distribution_chord_lengths(chord_lengths_set_13, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_circles(11, radius)\n",
    "chord_lengths_set_11 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_11 = distribution_chord_lengths(chord_lengths_set_11, num_bins) #discretised vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bce141-fd2e-4bfa-b2b6-c05b9696bd3d",
   "metadata": {},
   "source": [
    "## Classification Task 1: Inferring Sphere Density from Chord Distributions\n",
    "\n",
    "We aim to classify the density of spheres based on discretized chord length distributions obtained from horizontal cuts.\n",
    "\n",
    "- **Classes**: `distribution_11` (low density) vs. `distribution_17` (high density)\n",
    "- **Input**: a 15Ã—1 feature vector (discretized chord lengths)\n",
    "- **Goal**: predict which class (density level) the feature vector belongs to\n",
    "- **Parameters**: \n",
    "  - `num_bins = 15` for finer discretization\n",
    "\n",
    "This task mimics a supervised learning setting, where the model learns from labeled distributions and predicts the class of new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c551e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Flatten vectors and create (X, y) pairs\n",
    "def create_X_y(distribution, label):\n",
    "    X = flatten_discretized_vectors(distribution)\n",
    "    y = np.full(len(X), label)\n",
    "    return X, y\n",
    "\n",
    "X_11, y_11 = create_X_y(distribution_11, label=0)  # low density\n",
    "X_17, y_17 = create_X_y(distribution_17, label=1)  # high density\n",
    "\n",
    "# Combine training data\n",
    "X_all = np.vstack([X_11, X_17])\n",
    "y_all = np.concatenate([y_11, y_17])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2,random_state=42,stratify=y_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69a4cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cbc57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.71\n",
      "SVM Accuracy: 0.81\n",
      "KNN Accuracy: 0.77\n",
      "Random Forest Accuracy: 0.82\n",
      "XGBoost Accuracy: 0.81\n",
      "Naive Bayes Accuracy: 0.64\n",
      "MLP Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfb42d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.69 Â± 0.02\n",
      "SVM CV Accuracy: 0.80 Â± 0.02\n",
      "KNN CV Accuracy: 0.75 Â± 0.02\n",
      "Random Forest CV Accuracy: 0.80 Â± 0.02\n",
      "XGBoost CV Accuracy: 0.81 Â± 0.01\n",
      "Naive Bayes CV Accuracy: 0.54 Â± 0.04\n",
      "MLP CV Accuracy: 0.82 Â± 0.02\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, model in models.items():\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_all):\n",
    "        X_train_k, X_test_k = X_all[train_idx], X_all[val_idx]\n",
    "        y_train_k, y_test_k = y_all[train_idx], y_all[val_idx]\n",
    "\n",
    "        model.fit(X_train_k, y_train_k)\n",
    "        y_pred_k = model.predict(X_test_k)\n",
    "        acc = accuracy_score(y_test_k, y_pred_k)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    print(f\"{name} CV Accuracy: {np.mean(accuracies):.2f} Â± {np.std(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfd9d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Stratified CV Accuracy: 0.69 Â± 0.02\n",
      "SVM Stratified CV Accuracy: 0.80 Â± 0.01\n",
      "KNN Stratified CV Accuracy: 0.73 Â± 0.02\n",
      "Random Forest Stratified CV Accuracy: 0.81 Â± 0.01\n",
      "XGBoost Stratified CV Accuracy: 0.82 Â± 0.01\n",
      "Naive Bayes Stratified CV Accuracy: 0.53 Â± 0.02\n",
      "MLP Stratified CV Accuracy: 0.82 Â± 0.01\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_all, y_all):\n",
    "        X_train_k, X_test_k = X_all[train_idx], X_all[val_idx]\n",
    "        y_train_k, y_test_k = y_all[train_idx], y_all[val_idx]\n",
    "\n",
    "        model.fit(X_train_k, y_train_k)\n",
    "        y_pred_k = model.predict(X_test_k)\n",
    "        acc = accuracy_score(y_test_k, y_pred_k)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    print(f\"{name} Stratified CV Accuracy: {np.mean(accuracies):.2f} Â± {np.std(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312148c-fc65-446a-9b6e-ffcda8f97f1b",
   "metadata": {},
   "source": [
    "## Regression Task 2: Inferring Sphere Radius from Chord Distributions\n",
    "\n",
    "We now approach a **regression** problem: estimating the true **radius** of the spheres based on the shape of their chord length distributions.\n",
    "\n",
    "- **Classes (Radii)**: choose two known radii, e.g. `r = 0.04` and `r = 0.08`\n",
    "- **Input**: a 10Ã—1 discretized feature vector from chord lengths\n",
    "- **Output**: estimated radius of the sphere sample\n",
    "- **Goal**: train a regression model to learn the mapping from discretized distributions to true radius values\n",
    "\n",
    "This task demonstrates how statistical features derived from 2D cuts can be used to infer underlying 3D geometric properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c78b8dd-2250-4755-9157-8b34bafea5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### for you to fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67947d-8388-47ed-8433-d84aaea84b74",
   "metadata": {},
   "source": [
    "### ðŸ”§ Lifehack: Sampling a Realistic Discretized Vector to save your time\n",
    "\n",
    "This small code snippet is a quick way to extract a **realistic discretized vector** from a generated chord length distribution. It simulates a sphere arrangement with known radius and density, collects chord lengths from horizontal cuts, and selects the **first meaningful vector** (i.e., one that contains actual data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "079236ba-96fa-4a1a-8f2d-24ecd0b1b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample discretized vector: [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_circles = 17\n",
    "radius = 0.08\n",
    "num_cuts = 100\n",
    "num_bins = 15\n",
    "\n",
    "# Generate spheres and chord lengths\n",
    "positions = generate_circles(num_circles, radius)\n",
    "chord_lengths_set = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution = distribution_chord_lengths(chord_lengths_set, num_bins)\n",
    "\n",
    "# Extract a single discretized vector (e.g., the first non-empty one)\n",
    "sample_vector = None\n",
    "for vec in distribution:\n",
    "    if vec.sum() > 0:  # Skip empty cuts\n",
    "        sample_vector = vec\n",
    "        break\n",
    "\n",
    "# Show the result\n",
    "print(\"Sample discretized vector:\", sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a37b22-ecf4-4abf-9c6b-cb1da795d260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
