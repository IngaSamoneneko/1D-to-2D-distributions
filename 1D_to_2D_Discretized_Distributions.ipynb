{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63ff18b-7ba4-41e3-b517-17e21b2ee0a3",
   "metadata": {},
   "source": [
    "# ðŸŒ³ 1D to 2D Classification using Decision Trees\n",
    "\n",
    "This notebook demonstrates how to classify data by learning the mapping from 1D measurements (e.g., chord lengths) to 2D shape categories using decision tree algorithms. Such tasks arise in stereology and materials analysis where observed features are lower-dimensional projections of higher-dimensional structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940fa79-8683-4741-be5e-4cef2531a97f",
   "metadata": {},
   "source": [
    "## Inferring Sphere Radius from Chord Length Distributions\n",
    "\n",
    "This script simulates 2D cuts through randomly packed spheres and collects the resulting chord lengths. These are converted into binned histograms (feature vectors) used to identify the original sphere radius via pattern matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88298b47-fb9e-406b-9012-e90ab1659164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Generate non-overlapping spheres in a unit square\n",
    "def generate_circles(num_circles, radius):\n",
    "    positions = []\n",
    "    for _ in range(num_circles):\n",
    "        while True:\n",
    "            x = np.random.uniform(radius, 1 - radius)\n",
    "            y = np.random.uniform(radius, 1 - radius)\n",
    "            if all(np.sqrt((x - px) ** 2 + (y - py) ** 2) >= 2 * radius for px, py in positions):\n",
    "                positions.append((x, y))\n",
    "                break\n",
    "    return positions\n",
    "\n",
    "# Collect chord lengths from all circle for one secant (one cut-line)\n",
    "def calculate_chord_lengths_for_cut(a: float, b: float, \n",
    "                                  positions: List[Tuple[float, float]], \n",
    "                                  radius: float) -> List[float]:\n",
    "    return [calculate_chord_length(a, b, pos, radius) for pos in positions]\n",
    "\n",
    "# Calculate chord length for a single circle and line\n",
    "def calculate_chord_length(a: float, b: float, \n",
    "                         center: Tuple[float, float], \n",
    "                         radius: float) -> float:\n",
    "    x0, y0 = center\n",
    "    distance = abs(a*x0 - y0 + b)/np.sqrt(a**2 + 1)\n",
    "    return 2*np.sqrt(max(0, radius**2 - distance**2)) \n",
    "\n",
    "# Discretize a vector of chord lengths into bins\n",
    "def discretize_chords(chord_lengths: List[float], \n",
    "                      num_bins: int = 15, \n",
    "                      max_length: float = 0.16) -> np.ndarray:\n",
    "    bin_edges = np.linspace(0, max_length, num_bins + 1)\n",
    "    hist = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    for val in chord_lengths:\n",
    "        if val == 0.0:\n",
    "            continue            # this helps me to avoid collection of zeros in the first bin\n",
    "        if val == max_length:\n",
    "            hist[-1] += 1\n",
    "            continue\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] <= val < bin_edges[i + 1]:\n",
    "                hist[i] += 1\n",
    "                break\n",
    "\n",
    "    return hist\n",
    "\n",
    "# Generate multiple discretized vectors\n",
    "def generate_distribution(positions: List[Tuple[float, float]], \n",
    "                        radius: float, \n",
    "                        num_cuts: int, \n",
    "                        num_bins: int) -> List[np.ndarray]:\n",
    "    distribution = []\n",
    "    for _ in range(num_cuts):\n",
    "        #x1, y1 = np.random.rand(2)\n",
    "        #x2, y2 = np.random.rand(2)\n",
    "        #while x1 == x2:\n",
    "        #    x2 = np.random.rand()\n",
    "        #a = (y2 - y1) / (x2 - x1)\n",
    "        #b = y1 - a * x1\n",
    "        a = 0\n",
    "        b = np.random.uniform(0, 1)\n",
    "        chords = calculate_chord_lengths_for_cut(a, b, positions, radius)\n",
    "        discretized = discretize_chords(chords, num_bins)\n",
    "        distribution.append(discretized)\n",
    "    return distribution\n",
    "\n",
    "# this helps to test models - the sample is in the right copy-and-paste format\n",
    "def sample_from(distribution: List[np.ndarray]) -> Union[np.ndarray, None]:\n",
    "    \"\"\"Get first non-empty discretized vector\"\"\"\n",
    "    for vec in distribution:\n",
    "        if np.any(vec > 0):  # Check if any bin has counts\n",
    "            return vec\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff11d38-567e-490e-a97b-5cccdcf52384",
   "metadata": {},
   "source": [
    "## Simulating Chord Distributions for Different Sphere Densities\n",
    "\n",
    "We simulate 2D cross-sections through sets of non-overlapping spheres (with fixed radius) at four different densities. For each case, we compute chord lengths from horizontal cuts and discretize them into histograms for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d26d6d-0ece-46a0-9a8d-909f581de36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "NUM_CUTS = 1000  # Number of horizontal cuts\n",
    "NUM_BINS = 10 #feature vectors\n",
    "RADIUS = 0.08  # Radius of each circle\n",
    "\n",
    "positions_21 = generate_circles(21,RADIUS)\n",
    "distribution_21 = generate_distribution(positions_21, RADIUS, NUM_CUTS, NUM_BINS) #discretised vectors\n",
    "\n",
    "positions_11 = generate_circles(11, RADIUS)\n",
    "distribution_11 = generate_distribution(positions_11, RADIUS, NUM_CUTS, NUM_BINS) #discretised vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bce141-fd2e-4bfa-b2b6-c05b9696bd3d",
   "metadata": {},
   "source": [
    "## Classification Task 1: Inferring Sphere Density from Chord Distributions\n",
    "\n",
    "We aim to classify the density of spheres based on discretized chord length distributions obtained from horizontal cuts.\n",
    "\n",
    "- **Classes**: `distribution_11` (low density) vs. `distribution_21` (high density)\n",
    "- **Input**: a 15Ã—1 feature vector (discretized chord lengths)\n",
    "- **Goal**: predict which class (density level) the feature vector belongs to\n",
    "- **Parameters**: \n",
    "  - `num_bins = 10` for finer discretization\n",
    "\n",
    "This task mimics a supervised learning setting, where the model learns from labeled distributions and predicts the class of new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c551e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Flatten vectors and create (X, y) pairs\n",
    "def create_X_y(distribution, label):\n",
    "    X = distribution\n",
    "    y = np.full(len(X), label)\n",
    "    return X, y\n",
    "\n",
    "X_11, y_11 = create_X_y(distribution_11, label=0)  # low density\n",
    "X_17, y_17 = create_X_y(distribution_21, label=1)  # high density\n",
    "\n",
    "# Combine training data\n",
    "X_all = np.vstack([X_11, X_17])\n",
    "y_all = np.concatenate([y_11, y_17])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2,random_state=42,stratify=y_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a4cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbc57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.78\n",
      "SVM Accuracy: 0.83\n",
      "KNN Accuracy: 0.79\n",
      "Random Forest Accuracy: 0.83\n",
      "XGBoost Accuracy: 0.83\n",
      "Naive Bayes Accuracy: 0.70\n",
      "MLP Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfb42d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.78 Â± 0.01\n",
      "SVM CV Accuracy: 0.83 Â± 0.01\n",
      "KNN CV Accuracy: 0.75 Â± 0.01\n",
      "Random Forest CV Accuracy: 0.84 Â± 0.01\n",
      "XGBoost CV Accuracy: 0.84 Â± 0.01\n",
      "Naive Bayes CV Accuracy: 0.73 Â± 0.02\n",
      "MLP CV Accuracy: 0.85 Â± 0.01\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, model in models.items():\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_all):\n",
    "        X_train_k, X_test_k = X_all[train_idx], X_all[val_idx]\n",
    "        y_train_k, y_test_k = y_all[train_idx], y_all[val_idx]\n",
    "\n",
    "        model.fit(X_train_k, y_train_k)\n",
    "        y_pred_k = model.predict(X_test_k)\n",
    "        acc = accuracy_score(y_test_k, y_pred_k)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    print(f\"{name} CV Accuracy: {np.mean(accuracies):.2f} Â± {np.std(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd9d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Stratified CV Accuracy: 0.78 Â± 0.03\n",
      "SVM Stratified CV Accuracy: 0.83 Â± 0.02\n",
      "KNN Stratified CV Accuracy: 0.76 Â± 0.01\n",
      "Random Forest Stratified CV Accuracy: 0.83 Â± 0.02\n",
      "XGBoost Stratified CV Accuracy: 0.83 Â± 0.02\n",
      "Naive Bayes Stratified CV Accuracy: 0.72 Â± 0.03\n",
      "MLP Stratified CV Accuracy: 0.84 Â± 0.02\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_all, y_all):\n",
    "        X_train_k, X_test_k = X_all[train_idx], X_all[val_idx]\n",
    "        y_train_k, y_test_k = y_all[train_idx], y_all[val_idx]\n",
    "\n",
    "        model.fit(X_train_k, y_train_k)\n",
    "        y_pred_k = model.predict(X_test_k)\n",
    "        acc = accuracy_score(y_test_k, y_pred_k)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    print(f\"{name} Stratified CV Accuracy: {np.mean(accuracies):.2f} Â± {np.std(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fbe66db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Cross-Validation Accuracy: 0.84 Â± 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize Stratified K-Fold (5 splits)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create XGBoost classifier (with logloss for binary classification)\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for parameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,  # Using the same stratified splits\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform grid search on the entire dataset\n",
    "grid_search.fit(X_all, y_all)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate using stratified cross-validation\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_all, y_all):\n",
    "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "    \n",
    "    # Train with best parameters (refit=False would skip this)\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "    y_pred = best_xgb_model.predict(X_val)\n",
    "    cv_accuracies.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "# Print cross-validation results\n",
    "mean_accuracy = np.mean(cv_accuracies)\n",
    "std_accuracy = np.std(cv_accuracies)\n",
    "print(f\"Cross-Validation Accuracy: {mean_accuracy:.2f} Â± {std_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22b16489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP Parameters: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01}\n",
      "Best CV Score (from grid search): 0.8385\n",
      "\n",
      "Independent Cross-Validation Results:\n",
      "Accuracy: 0.8385 Â± 0.0167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grid for MLP\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # L2 regularization\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Initialize Stratified K-Fold (consistent with XGBoost setup)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create MLP classifier with fixed parameters\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,  # Recommended for neural networks\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_mlp = GridSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_grid=param_grid_mlp,\n",
    "    cv=skf,  # Use the same stratified splits\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform grid search on full data (X_all, y_all)\n",
    "grid_mlp.fit(X_all, y_all)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_mlp_params = grid_mlp.best_params_\n",
    "best_mlp_model = grid_mlp.best_estimator_\n",
    "\n",
    "print(\"Best MLP Parameters:\", best_mlp_params)\n",
    "print(\"Best CV Score (from grid search):\", grid_mlp.best_score_)\n",
    "\n",
    "# Independent stratified cross-validation evaluation\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_all, y_all):\n",
    "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "    \n",
    "    # Train with best parameters (fresh instance)\n",
    "    best_mlp_model.fit(X_train, y_train)\n",
    "    y_pred = best_mlp_model.predict(X_val)\n",
    "    cv_accuracies.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mean_accuracy = np.mean(cv_accuracies)\n",
    "std_accuracy = np.std(cv_accuracies)\n",
    "\n",
    "print(\"\\nIndependent Cross-Validation Results:\")\n",
    "print(f\"Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5934e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'class_weight': None, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Grid Search CV Accuracy: 0.8414999999999999\n",
      "\n",
      "Independent Validation Results:\n",
      "Accuracy: 0.8415 Â± 0.0243\n",
      "Average Feature Importances: [0.00695848 0.01538156 0.03849891 0.03065802 0.05799528 0.06699828\n",
      " 0.07251051 0.15153295 0.1786017  0.38086431]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],  # None allows full tree expansion\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],  # Recommended for classification\n",
    "    'class_weight': [None, 'balanced']  # Handles imbalanced data\n",
    "}\n",
    "\n",
    "# Initialize Stratified K-Fold (consistent with previous models)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Utilize all CPU cores\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1  # Parallelize grid search\n",
    ")\n",
    "\n",
    "# Full hyperparameter search\n",
    "grid_rf.fit(X_all, y_all)\n",
    "\n",
    "# Best results\n",
    "best_rf_params = grid_rf.best_params_\n",
    "best_rf_model = grid_rf.best_estimator_\n",
    "\n",
    "print(\"Best Random Forest Parameters:\", best_rf_params)\n",
    "print(\"Grid Search CV Accuracy:\", grid_rf.best_score_)\n",
    "\n",
    "# Independent stratified cross-validation\n",
    "cv_accuracies = []\n",
    "feature_importances = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_all, y_all):\n",
    "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "    \n",
    "    # Train fresh model with best parameters\n",
    "    best_rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = best_rf_model.predict(X_val)\n",
    "    cv_accuracies.append(accuracy_score(y_val, y_pred))\n",
    "    \n",
    "    # Track feature importances\n",
    "    feature_importances.append(best_rf_model.feature_importances_)\n",
    "\n",
    "# Calculate metrics\n",
    "mean_accuracy = np.mean(cv_accuracies)\n",
    "std_accuracy = np.std(cv_accuracies)\n",
    "mean_feature_importance = np.mean(feature_importances, axis=0)\n",
    "\n",
    "print(\"\\nIndependent Validation Results:\")\n",
    "print(f\"Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
    "print(\"Average Feature Importances:\", mean_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b60c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predictions:\n",
      "Random Forest: 1\n",
      "XGBoost: 1\n",
      "MLP: 1\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Random Forest\": best_rf_model,\n",
    "    \"XGBoost\": best_xgb_model,\n",
    "    \"MLP\": best_mlp_model\n",
    "}\n",
    "\n",
    "sample_vector_21 = None\n",
    "for vec in distribution_21:\n",
    "    if vec.sum() > 0:  # Skip empty cuts\n",
    "        # Convert to numpy array and ensure proper numeric type\n",
    "        sample_vector_21 = np.array(vec, dtype=np.float32).reshape(1, -1)\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model Predictions:\")\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        pred = model.predict(sample_vector_21)\n",
    "        print(f\"{name}: {pred[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312148c-fc65-446a-9b6e-ffcda8f97f1b",
   "metadata": {},
   "source": [
    "## Regression Task 2: Inferring Sphere Radius from Chord Distributions\n",
    "\n",
    "We now approach a **regression** problem: estimating the true **radius** of the spheres based on the shape of their chord length distributions.\n",
    "\n",
    "- **Classes (Radii)**: choose two known radii, e.g. `r = 0.04` and `r = 0.08`\n",
    "- **Input**: a 10Ã—1 discretized feature vector from chord lengths\n",
    "- **Output**: estimated radius of the sphere sample\n",
    "- **Goal**: train a regression model to learn the mapping from discretized distributions to true radius values\n",
    "\n",
    "This task demonstrates how statistical features derived from 2D cuts can be used to infer underlying 3D geometric properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c78b8dd-2250-4755-9157-8b34bafea5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### for you to fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67947d-8388-47ed-8433-d84aaea84b74",
   "metadata": {},
   "source": [
    "### ðŸ”§ Lifehack: Sampling a Realistic Discretized Vector to save your time\n",
    "\n",
    "This small code snippet is a quick way to extract a **realistic discretized vector** from a generated chord length distribution. It simulates a sphere arrangement with known radius and density, collects chord lengths from horizontal cuts, and selects the **first meaningful vector** (i.e., one that contains actual data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079236ba-96fa-4a1a-8f2d-24ecd0b1b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample discretized vector: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_circles = 17\n",
    "radius = 0.08\n",
    "num_cuts = 100\n",
    "num_bins = 15\n",
    "\n",
    "# Generate spheres and chord lengths\n",
    "positions = generate_circles(num_circles, radius)\n",
    "chord_lengths_set = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution = distribution_chord_lengths(chord_lengths_set, num_bins)\n",
    "\n",
    "# Extract a single discretized vector (e.g., the first non-empty one)\n",
    "sample_vector = None\n",
    "for vec in distribution:\n",
    "    if vec.sum() > 0:  # Skip empty cuts\n",
    "        sample_vector = vec\n",
    "        break\n",
    "\n",
    "# Show the result\n",
    "print(\"Sample discretized vector:\", sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73a37b22-ecf4-4abf-9c6b-cb1da795d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Predictions:\n",
      "----------------\n",
      "Logistic Regression: 0\n",
      "SVM: 0\n",
      "KNN: 0\n",
      "Random Forest: 0\n",
      "XGBoost: 0\n",
      "Naive Bayes: 0\n",
      "MLP: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure sample_vector is in correct shape (1, n_features)\n",
    "sample_vector = np.array(sample_vector).reshape(1, -1)\n",
    "\n",
    "# Standardize the vector using the same scaler used during training\n",
    "# (Assuming you scaled your training data - if not, skip this step)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_all)  # X_all should be your original training data\n",
    "sample_vector_scaled = scaler.transform(sample_vector)\n",
    "\n",
    "# Dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Make predictions with each model\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        pred = model.predict(sample_vector_scaled)\n",
    "        proba = model.predict_proba(sample_vector_scaled) if hasattr(model, \"predict_proba\") else [None]\n",
    "        predictions[name] = {\n",
    "            'class': pred[0],\n",
    "            'probability': proba[0] if proba[0] is not None else \"N/A\",\n",
    "            'confidence': max(proba[0]) if proba[0] is not None else \"N/A\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        predictions[name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nModel Predictions:\")\n",
    "print(\"----------------\")\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        pred = model.predict(sample_vector)\n",
    "        print(f\"{name}: {pred[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f61d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
