{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnl3DYHlo6mqjhH1Hf11sI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IngaSamoneneko/1D-to-2D-distributions/blob/main/1D-to-2D%20densities%2085%20accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "ojLGLvkeYweJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate non-overlapping circles in a unit square\n",
        "def generate_circles(num_circles, radius):\n",
        "    positions = []\n",
        "    for _ in range(num_circles):\n",
        "        while True:\n",
        "            x = np.random.uniform(radius, 1 - radius)\n",
        "            y = np.random.uniform(radius, 1 - radius)\n",
        "            if all(np.sqrt((x - px) ** 2 + (y - py) ** 2) >= 2 * radius for px, py in positions):\n",
        "                positions.append((x, y))\n",
        "                break\n",
        "    return positions\n",
        "\n",
        "# Collect chord lengths from all circle for one secant (one cut-line)\n",
        "def calculate_chord_lengths_for_cut(a: float, b: float,\n",
        "                                  positions: List[Tuple[float, float]],\n",
        "                                  radius: float) -> List[float]:\n",
        "    return [calculate_chord_length(a, b, pos, radius) for pos in positions]\n",
        "\n",
        "# Calculate chord length for a single circle and line\n",
        "def calculate_chord_length(a: float, b: float,\n",
        "                         center: Tuple[float, float],\n",
        "                         radius: float) -> float:\n",
        "    x0, y0 = center\n",
        "    distance = abs(a*x0 - y0 + b)/np.sqrt(a**2 + 1)\n",
        "    return 2*np.sqrt(max(0, radius**2 - distance**2))\n",
        "\n",
        "# Discretize a vector of chord lengths into bins\n",
        "def discretize_chords(chord_lengths: List[float],\n",
        "                      num_bins: int = 15,\n",
        "                      max_length: float = 0.16) -> np.ndarray:\n",
        "    bin_edges = np.linspace(0, max_length, num_bins + 1)\n",
        "    hist = np.zeros(num_bins, dtype=int)\n",
        "\n",
        "    for val in chord_lengths:\n",
        "        if val == 0.0:\n",
        "            continue            # this helps me to avoid collection of zeros in the first bin\n",
        "        if val == max_length:\n",
        "            hist[-1] += 1\n",
        "            continue\n",
        "        for i in range(num_bins):\n",
        "            if bin_edges[i] <= val < bin_edges[i + 1]:\n",
        "                hist[i] += 1\n",
        "                break\n",
        "\n",
        "    return hist\n",
        "\n",
        "# Generate multiple discretized vectors\n",
        "def generate_distribution(positions: List[Tuple[float, float]],\n",
        "                        radius: float,\n",
        "                        num_cuts: int,\n",
        "                        num_bins: int) -> List[np.ndarray]:\n",
        "    distribution = []\n",
        "    for _ in range(num_cuts):\n",
        "        #x1, y1 = np.random.rand(2)\n",
        "        #x2, y2 = np.random.rand(2)\n",
        "        #while x1 == x2:\n",
        "        #    x2 = np.random.rand()\n",
        "        #a = (y2 - y1) / (x2 - x1)\n",
        "        #b = y1 - a * x1\n",
        "        a = 0\n",
        "        b = np.random.uniform(0, 1)\n",
        "        chords = calculate_chord_lengths_for_cut(a, b, positions, radius)\n",
        "        discretized = discretize_chords(chords, num_bins)\n",
        "        distribution.append(discretized)\n",
        "    return distribution\n",
        "\n",
        "# this helps to test models - the sample is in the right copy-and-paste format\n",
        "def sample_from(distribution: List[np.ndarray]) -> Union[np.ndarray, None]:\n",
        "    \"\"\"Get first non-empty discretized vector\"\"\"\n",
        "    for vec in distribution:\n",
        "        if np.any(vec > 0):  # Check if any bin has counts\n",
        "            return vec\n",
        "    return None"
      ],
      "metadata": {
        "id": "k5g_cx1AVQ3N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "NUM_CUTS = 1000  # Number of horizontal cuts\n",
        "NUM_BINS = 10 #feature vectors\n",
        "RADIUS = 0.08  # Radius of each circle\n",
        "\n",
        "positions_21 = generate_circles(21,RADIUS)\n",
        "distribution_21 = generate_distribution(positions_21, RADIUS, NUM_CUTS, NUM_BINS) #discretised vectors\n",
        "\n",
        "positions_11 = generate_circles(11, RADIUS)\n",
        "distribution_11 = generate_distribution(positions_11, RADIUS, NUM_CUTS, NUM_BINS) #discretised vectors"
      ],
      "metadata": {
        "id": "3Dm8e99cVhMl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten vectors and create (X, y) pairs\n",
        "def create_X_y(distribution, label):\n",
        "    X = distribution\n",
        "    y = np.full(len(X), label)\n",
        "    return X, y\n",
        "\n",
        "X_11, y_11 = create_X_y(distribution_11, label=0)  # low density\n",
        "X_17, y_17 = create_X_y(distribution_21, label=1)  # high density\n",
        "\n",
        "# Combine training data\n",
        "X_all = np.vstack([X_11, X_17])\n",
        "y_all = np.concatenate([y_11, y_17])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2,random_state=42,stratify=y_all)"
      ],
      "metadata": {
        "id": "OnYs96ynVoOp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": xgb.XGBClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000)\n",
        "}"
      ],
      "metadata": {
        "id": "2wmoXLGtVqjh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp3u4JVTVtYy",
        "outputId": "3da93849-a1f6-43ad-ca39-03ac1b69c422"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.83\n",
            "SVM Accuracy: 0.86\n",
            "KNN Accuracy: 0.82\n",
            "Random Forest Accuracy: 0.86\n",
            "XGBoost Accuracy: 0.85\n",
            "Naive Bayes Accuracy: 0.72\n",
            "MLP Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models.items():\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_all, y_all):\n",
        "        X_train_k, X_test_k = X_all[train_idx], X_all[val_idx]\n",
        "        y_train_k, y_test_k = y_all[train_idx], y_all[val_idx]\n",
        "\n",
        "        model.fit(X_train_k, y_train_k)\n",
        "        y_pred_k = model.predict(X_test_k)\n",
        "        acc = accuracy_score(y_test_k, y_pred_k)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    print(f\"{name} Stratified CV Accuracy: {np.mean(accuracies):.2f} ± {np.std(accuracies):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btrh2QikV0tC",
        "outputId": "28be4fc6-3b61-4395-a282-9c52ad37fbbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Stratified CV Accuracy: 0.85 ± 0.02\n",
            "SVM Stratified CV Accuracy: 0.87 ± 0.01\n",
            "KNN Stratified CV Accuracy: 0.77 ± 0.01\n",
            "Random Forest Stratified CV Accuracy: 0.88 ± 0.02\n",
            "XGBoost Stratified CV Accuracy: 0.87 ± 0.01\n",
            "Naive Bayes Stratified CV Accuracy: 0.75 ± 0.02\n",
            "MLP Stratified CV Accuracy: 0.87 ± 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ASSUME X_all, y_all ARE ALREADY DEFINED\n",
        "# ==========================\n",
        "# Stratified 5-fold setup\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ==========================\n",
        "# DEFINE MODELS AND THEIR PARAM GRIDS\n",
        "# ==========================\n",
        "tuning_dict = {\n",
        "    \"XGBoost\": {\n",
        "        \"estimator\": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42),\n",
        "        \"param_grid\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "    },\n",
        "    \"MLP\": {\n",
        "        \"estimator\": MLPClassifier(max_iter=1000, early_stopping=True, random_state=42),\n",
        "        \"param_grid\": {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'activation': ['relu', 'tanh'],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "            'learning_rate_init': [0.001, 0.01]\n",
        "        }\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"estimator\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        \"param_grid\": {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'max_features': ['sqrt', 'log2'],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==========================\n",
        "# RUN GRID SEARCH FOR EACH MODEL\n",
        "# ==========================\n",
        "results = {}\n",
        "\n",
        "for name, cfg in tuning_dict.items():\n",
        "    print(f\"\\n=== Tuning {name} ===\")\n",
        "\n",
        "    model = cfg[\"estimator\"]\n",
        "    grid = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=cfg[\"param_grid\"],\n",
        "        cv=skf,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    grid.fit(X_all, y_all)\n",
        "\n",
        "    best_params = grid.best_params_\n",
        "    best_score = grid.best_score_\n",
        "    best_est = grid.best_estimator_\n",
        "\n",
        "    print(f\"Best {name} Params: {best_params}\")\n",
        "    print(f\"Best {name} CV Accuracy: {best_score:.4f}\")\n",
        "\n",
        "    # Optional independent stratified CV to confirm\n",
        "    cv_accs = []\n",
        "    for train_idx, val_idx in skf.split(X_all, y_all):\n",
        "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
        "\n",
        "        # Fit a fresh clone to avoid any side effects\n",
        "        best_clone = best_est.__class__(**best_params, random_state=42)\n",
        "        best_clone.fit(X_train, y_train)\n",
        "        preds = best_clone.predict(X_val)\n",
        "        cv_accs.append(accuracy_score(y_val, preds))\n",
        "\n",
        "    mean_acc, std_acc = np.mean(cv_accs), np.std(cv_accs)\n",
        "    print(f\"Independent Stratified CV: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"best_params\": best_params,\n",
        "        \"grid_cv_score\": best_score,\n",
        "        \"independent_cv\": (mean_acc, std_acc)\n",
        "    }\n",
        "\n",
        "# ==========================\n",
        "# ALL RESULTS STORED IN 'results' DICT\n",
        "# ==========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCrC-FBKi7tx",
        "outputId": "f5846883-b4d4-4d71-bf10-3d116a16e731"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tuning XGBoost ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:31:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best XGBoost Params: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Best XGBoost CV Accuracy: 0.8730\n",
            "Independent Stratified CV: 0.8730 ± 0.0181\n",
            "\n",
            "=== Tuning MLP ===\n",
            "Best MLP Params: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01}\n",
            "Best MLP CV Accuracy: 0.8755\n",
            "Independent Stratified CV: 0.8735 ± 0.0215\n",
            "\n",
            "=== Tuning Random Forest ===\n",
            "Best Random Forest Params: {'class_weight': None, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Best Random Forest CV Accuracy: 0.8770\n",
            "Independent Stratified CV: 0.8770 ± 0.0170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "n = 200  # number of samples to generate\n",
        "results = []\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": best_rf_model,\n",
        "    \"XGBoost\": best_xgb_model,\n",
        "    \"MLP\": best_mlp_model\n",
        "}\n",
        "\n",
        "ones_count = {name: 0 for name in models}\n",
        "\n",
        "for i in range(n):\n",
        "    # Step 1: Generate new positions and one cut\n",
        "    positions = generate_circles(21, 0.08)\n",
        "    vec = generate_distribution(positions, 0.08, 1, NUM_BINS)\n",
        "\n",
        "    sample_vector = np.array(vec, dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "    # Step 3: Run model predictions\n",
        "    row = {\"Sample #\": i + 1}\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            pred = model.predict(sample_vector)\n",
        "            row[name] = pred[0]\n",
        "            if pred[0] == 1:\n",
        "                ones_count[name] += 1\n",
        "        except Exception as e:\n",
        "            row[name] = f\"Error: {str(e)}\"\n",
        "\n",
        "    results.append(row)\n",
        "\n",
        "# Step 4: Convert to DataFrame and show\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Step 5: Append ones count as summary row\n",
        "summary_row = {\"Sample #\": \"Total 1s\"}\n",
        "summary_row.update({name: (ones_count[name]/n) for name in models})\n",
        "df = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR3_MSYCWcEH",
        "outputId": "eaaeba05-0125-46ce-f2fb-1ca53e2f2f02"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Sample #  Random Forest  XGBoost    MLP\n",
            "0           1           0.00     0.00  0.000\n",
            "1           2           1.00     1.00  1.000\n",
            "2           3           1.00     1.00  1.000\n",
            "3           4           1.00     1.00  1.000\n",
            "4           5           1.00     1.00  1.000\n",
            "..        ...            ...      ...    ...\n",
            "196       197           0.00     0.00  0.000\n",
            "197       198           1.00     1.00  1.000\n",
            "198       199           1.00     1.00  1.000\n",
            "199       200           1.00     1.00  1.000\n",
            "200  Total 1s           0.78     0.77  0.795\n",
            "\n",
            "[201 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1) Define StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 2) Combine both models and their parameter distributions into one dict\n",
        "search_space = [\n",
        "    {\n",
        "        'model': [RandomForestClassifier(random_state=42)],\n",
        "        'model__n_estimators': [100, 200, 300],\n",
        "        'model__max_depth': [None, 10, 20],\n",
        "        'model__min_samples_split': [2, 5],\n",
        "        'model__max_features': ['sqrt', 'log2'],\n",
        "        'model__class_weight': [None, 'balanced']\n",
        "    },\n",
        "    {\n",
        "        'model': [XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)],\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [3, 5, 7],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__subsample': [0.8, 1.0]\n",
        "    }\n",
        "]\n",
        "\n",
        "# 3) Wrap models in a simple Pipeline (even if no preprocessing)\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipe = Pipeline([('model', RandomForestClassifier())])\n",
        "\n",
        "# 4) Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_distributions=search_space,\n",
        "    n_iter=20,                      # limit to 20 random combos (instead of full grid)\n",
        "    scoring='accuracy',\n",
        "    cv=skf,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5) Run the search\n",
        "random_search.fit(X_all, y_all)\n",
        "\n",
        "# 6) Extract best results\n",
        "best_model = random_search.best_estimator_.named_steps['model']\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "print(\"Best Model:\", best_model.__class__.__name__)\n",
        "print(\"Best Params:\", best_params)\n",
        "print(f\"Best Stratified CV Accuracy: {best_score:.4f}\")\n",
        "\n",
        "# 7) Final independent Stratified CV check\n",
        "cv_accs = []\n",
        "for train_idx, val_idx in skf.split(X_all, y_all):\n",
        "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
        "    clone = best_model.__class__(**{k.replace('model__',''): v for k,v in best_params.items() if k.startswith('model__')})\n",
        "    clone.fit(X_train, y_train)\n",
        "    preds = clone.predict(X_val)\n",
        "    cv_accs.append(accuracy_score(y_val, preds))\n",
        "\n",
        "mean_acc, std_acc = np.mean(cv_accs), np.std(cv_accs)\n",
        "print(f\"Independent CV Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-OXVmzyW72K",
        "outputId": "9ea97039-58e3-4a6e-abfa-736b3c0d77c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Model: RandomForestClassifier\n",
            "Best Params: {'model__n_estimators': 200, 'model__min_samples_split': 2, 'model__max_features': 'log2', 'model__max_depth': 20, 'model__class_weight': 'balanced', 'model': RandomForestClassifier(random_state=42)}\n",
            "Best Stratified CV Accuracy: 0.8770\n",
            "Independent CV Accuracy: 0.8760 ± 0.0176\n"
          ]
        }
      ]
    }
  ]
}