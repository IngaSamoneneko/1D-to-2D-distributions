{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63ff18b-7ba4-41e3-b517-17e21b2ee0a3",
   "metadata": {},
   "source": [
    "# üå≥ 1D to 2D Classification using Decision Trees\n",
    "\n",
    "This notebook demonstrates how to classify data by learning the mapping from 1D measurements (e.g., chord lengths) to 2D shape categories using decision tree algorithms. Such tasks arise in stereology and materials analysis where observed features are lower-dimensional projections of higher-dimensional structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940fa79-8683-4741-be5e-4cef2531a97f",
   "metadata": {},
   "source": [
    "## Inferring Sphere Radius from Chord Length Distributions\n",
    "\n",
    "This script simulates 2D cuts through randomly packed spheres and collects the resulting chord lengths. These are converted into binned histograms (feature vectors) used to identify the original sphere radius via pattern matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88298b47-fb9e-406b-9012-e90ab1659164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def generate_spheres(num_spheres, radius):\n",
    "    positions = []\n",
    "    for _ in range(num_spheres):\n",
    "        while True:\n",
    "            x = np.random.uniform(radius, 1 - radius)\n",
    "            y = np.random.uniform(radius, 1 - radius)\n",
    "            if all(np.sqrt((x - px) ** 2 + (y - py) ** 2) >= 2 * radius for px, py in positions):\n",
    "                positions.append((x, y))\n",
    "                break\n",
    "    return positions\n",
    "\n",
    "def chord_length(radius, d_line):\n",
    "    return 2 * np.sqrt(radius**2 - d_line**2)\n",
    "\n",
    "def distance_from_line(a, b, x, y):\n",
    "    return abs(a * x - y + b) / np.sqrt(a**2 + 1)\n",
    "\n",
    "def collect_chords_for_cuts(positions, radius, num_cuts, y_range=(0, 1)):\n",
    "    chord_lengths_per_cut = []\n",
    "    # Spacing for horizontal cuts\n",
    "    y_cut_values = np.linspace(y_range[0], y_range[1], num_cuts) \n",
    "    # Collect chord lengths for each cut\n",
    "    for y_cut in y_cut_values:\n",
    "        cut_chords = []\n",
    "        for (x, y) in positions:\n",
    "            # Calculate the perpendicular distance from the sphere center to the cut\n",
    "            d_line = abs(y - y_cut)  # This is simply the vertical distance since it's a horizontal line\n",
    "\n",
    "            # Check if the line intersects the sphere (distance must be less than the radius)\n",
    "            if d_line < radius:\n",
    "                length = chord_length(radius, d_line)\n",
    "                cut_chords.append(length)      \n",
    "        # Sort the chords for the current cut\n",
    "        chord_lengths_per_cut.append(sorted(cut_chords))\n",
    "    \n",
    "    return chord_lengths_per_cut\n",
    "\n",
    "\n",
    "def discretize_chord_lengths(chord_lengths, num_bins, range_min = 0, range_max=0.16):\n",
    "    bin_edges = np.linspace(range_min, range_max, num_bins + 1)\n",
    "    \n",
    "    # Initialize a list to store the count of chord lengths in each bin\n",
    "    bin_counts = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    # For each chord length, find which bin it belongs to and increment the count for that bin\n",
    "    for chord in chord_lengths:\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] < chord <= bin_edges[i + 1]:\n",
    "                bin_counts[i] += 1\n",
    "                break\n",
    "\n",
    "    return bin_counts\n",
    "    \n",
    "\n",
    "def distribution_chord_lengths(chord_lengths_per_cut, num_bins):\n",
    "    discretized_vectors = []\n",
    "    \n",
    "    for chord_lengths in chord_lengths_per_cut:\n",
    "        if chord_lengths:  # If it's not empty, discretize\n",
    "            discretized_vector = discretize_chord_lengths(chord_lengths, num_bins)\n",
    "        #else:  # If it's empty, create a zeroed vector of the same length as the bins\n",
    "        #    discretized_vector = np.zeros(num_bins, dtype=int)\n",
    "        #\n",
    "            discretized_vectors.append(discretized_vector)\n",
    "    \n",
    "    return discretized_vectors\n",
    "\n",
    "\n",
    "def flatten_discretized_vectors(discretized_vectors):\n",
    "    # Flatten each discretized vector into a 1D feature vector\n",
    "    return np.array([vec.flatten() for vec in discretized_vectors])\n",
    "\n",
    "def create_feature_radius_pairs(distributions, true_radii):\n",
    "    # Ensure the number of distributions matches the number of true radii\n",
    "    if len(distributions) != len(true_radii):\n",
    "        raise ValueError(\"The number of distributions must match the number of true radii.\")\n",
    "    \n",
    "    # Flatten the distributions\n",
    "    flattened_distributions = [flatten_discretized_vectors(distribution) for distribution in distributions]\n",
    "    \n",
    "    # Pair each flattened vector with its corresponding true radius\n",
    "    feature_radius_pairs = []\n",
    "    for i, flattened_distribution in enumerate(flattened_distributions):\n",
    "        for feature_vector in flattened_distribution:\n",
    "            feature_radius_pairs.append((feature_vector, true_radii[i]))\n",
    "    \n",
    "    return feature_radius_pairs\n",
    "\n",
    "def compute_vector_frequencies(distribution):\n",
    "    vector_counts = defaultdict(int)\n",
    "\n",
    "    for vector in distribution:\n",
    "        vector_tuple = tuple(vector)  # Convert numpy array to tuple for hashing\n",
    "        vector_counts[vector_tuple] += 1\n",
    "\n",
    "    return vector_counts\n",
    "\n",
    "\n",
    "def discretize_vector(vector, num_bins=10, range_min=0, range_max=0.16):\n",
    "    # Discretize the observed vector using the same bins as the training data\n",
    "    bin_edges = np.linspace(range_min, range_max, num_bins + 1)\n",
    "    discretized_vector = np.zeros(num_bins, dtype=int)\n",
    "\n",
    "    for val in vector:\n",
    "        for i in range(num_bins):\n",
    "            if bin_edges[i] < val <= bin_edges[i + 1]:\n",
    "                discretized_vector[i] += 1\n",
    "                break\n",
    "\n",
    "    return tuple(discretized_vector)  # Convert to tuple for comparison in dictionary\n",
    "\n",
    "# Function to find the best match across multiple dictionaries\n",
    "def find_best_match_in_dictionaries(observed_vector, dictionaries, num_bins=10, range_min=0, range_max=0.16):\n",
    "    # Discretize the observed vector\n",
    "    discretized_observed = discretize_vector(observed_vector, num_bins, range_min, range_max)\n",
    "    #discretized_observed = (np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0))\n",
    "    best_match = None\n",
    "    best_frequency = 0\n",
    "    best_dict_name = None\n",
    "\n",
    "    # Loop through each dictionary and check for matches\n",
    "    for dict_name, vector_frequencies in dictionaries.items():\n",
    "        # Find matches in the current dictionary\n",
    "        matching_frequencies = {vec: count for vec, count in vector_frequencies.items() if vec == discretized_observed}\n",
    "        \n",
    "        if matching_frequencies:\n",
    "            # Find the highest frequency in this dictionary\n",
    "            max_frequency_in_dict = max(matching_frequencies.values())\n",
    "            \n",
    "            if max_frequency_in_dict > best_frequency:\n",
    "                best_frequency = max_frequency_in_dict\n",
    "                best_match = discretized_observed\n",
    "                best_dict_name = dict_name\n",
    "\n",
    "    return best_match, best_frequency, best_dict_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff11d38-567e-490e-a97b-5cccdcf52384",
   "metadata": {},
   "source": [
    "## Simulating Chord Distributions for Different Sphere Densities\n",
    "\n",
    "We simulate 2D cross-sections through sets of non-overlapping spheres (with fixed radius) at four different densities. For each case, we compute chord lengths from horizontal cuts and discretize them into histograms for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75d26d6d-0ece-46a0-9a8d-909f581de36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_cuts = 100  # Number of horizontal cuts\n",
    "num_bins = 10 #feature vectors\n",
    "\n",
    "radius = 0.08  # Radius of each circle\n",
    "\n",
    "positions = generate_spheres(17, radius)\n",
    "chord_lengths_set_17 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_17 = distribution_chord_lengths(chord_lengths_set_17, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_spheres(15, radius)\n",
    "chord_lengths_set_15 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_15 = distribution_chord_lengths(chord_lengths_set_15, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_spheres(13, radius)\n",
    "chord_lengths_set_13 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_13 = distribution_chord_lengths(chord_lengths_set_13, num_bins) #discretised vectors\n",
    "\n",
    "\n",
    "positions = generate_spheres(11, radius)\n",
    "chord_lengths_set_11 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_11 = distribution_chord_lengths(chord_lengths_set_11, num_bins) #discretised vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04165def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_numpy_dataset(num_spheres_list, radius, num_cuts, num_bins):\n",
    "    dataset = []\n",
    "\n",
    "    for num_spheres in num_spheres_list:\n",
    "        positions = generate_spheres(num_spheres, radius)\n",
    "        chord_lengths_set = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "        discretized_vectors = distribution_chord_lengths(chord_lengths_set, num_bins)\n",
    "\n",
    "        for vector in discretized_vectors:\n",
    "            dataset.append(list(vector) + [num_spheres])  # –¥–æ–¥–∞—î–º–æ –º—ñ—Ç–∫—É –Ω–∞–ø—Ä–∏–∫—ñ–Ω—Ü—ñ\n",
    "\n",
    "    data = np.array(dataset, dtype=np.float32)  # –º–æ–∂–Ω–∞ —Ç–∞–∫–æ–∂ int, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n",
    "    X = data[:, :-1]  # –≤—Å—ñ –æ–∑–Ω–∞–∫–∏\n",
    "    Y = data[:, -1]   # –≤—Å—ñ –º—ñ—Ç–∫–∏ (–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ñ–µ—Ä)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad0a7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, test_size=0.2, shuffle=True):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        Y = Y[indices]\n",
    "\n",
    "    split_point = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:split_point], X[split_point:]\n",
    "    Y_train, Y_test = Y[:split_point], Y[split_point:]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7a44d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self,x_data,y_data,numb_of_tree,depth):\n",
    "        self.x_data=x_data\n",
    "        self.y_data=y_data\n",
    "        self.numb_of_tree=numb_of_tree\n",
    "        self.depth=depth\n",
    "\n",
    "    def find_best_split(self,X, y, feature_indices):\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "\n",
    "        for feat in feature_indices:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for t in thresholds:\n",
    "                left_mask = X[:, feat] < t\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini = self.gini_split(y[left_mask], y[right_mask])\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feat\n",
    "                    best_thresh = t\n",
    "\n",
    "        return best_feature, best_thresh\n",
    "    def gini_split(self,y_left, y_right):\n",
    "        def gini(y):\n",
    "            classes, counts = np.unique(y, return_counts=True)\n",
    "            probs = counts / counts.sum()\n",
    "            return 1 - np.sum(probs ** 2)\n",
    "        n = len(y_left) + len(y_right)\n",
    "        return (len(y_left) / n) * gini(y_left) + (len(y_right) / n) * gini(y_right)\n",
    "    def predict_tree(self,x, node):\n",
    "        if node[\"is_leaf\"]:\n",
    "            return node[\"prediction\"]\n",
    "        if x[node[\"feature\"]] < node[\"threshold\"]:\n",
    "            return self.predict_tree(x, node[\"left\"])\n",
    "        else:\n",
    "            return self.predict_tree(x, node[\"right\"])\n",
    "    def random_feature(self,numb_of_feat,size):\n",
    "        arr1=[x for x in range(numb_of_feat)]\n",
    "        unique_numbers = np.unique(arr1)\n",
    "        result=np.random.choice(unique_numbers,size=size,replace=False)\n",
    "        return result\n",
    "    def most_common(self,y):\n",
    "        values, counts = np.unique(y, return_counts=True)  # –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ —ó—Ö –∫—ñ–ª—å–∫—ñ—Å—Ç—å\n",
    "        most_common_value = values[np.argmax(counts)]      # –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é\n",
    "        return most_common_value\n",
    "    def build_tree(self,X, y, feature_indices, depth=0):\n",
    "        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ –≤—Å—ñ –º—ñ—Ç–∫–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ ‚Üí –∑—É–ø–∏–Ω—è—î–º–æ—Å—å\n",
    "        if np.all(y == y[0]):\n",
    "            return {'is_leaf': True, 'prediction': y[0]}\n",
    "    \n",
    "        if depth == self.depth:\n",
    "            majority_class = self.most_common(y)\n",
    "            return {'is_leaf': True, 'prediction': majority_class}\n",
    "\n",
    "        # 3. –®—É–∫–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â–µ —Ä–æ–∑–±–∏—Ç—Ç—è\n",
    "        best_feature, best_thresh = self.find_best_split(X, y, feature_indices)\n",
    "        if best_feature is None or best_thresh is None:\n",
    "            majority_class = self.most_common(y)  # –ë–∞–∑—É—î–º–æ—Å—è –Ω–∞ –±—ñ–ª—å—à–æ—Å—Ç—ñ —É –ø–æ—Ç–æ—á–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
    "            return {'is_leaf': True, 'prediction': majority_class}\n",
    "        # 4. –†–æ–∑–¥—ñ–ª—è—î–º–æ –¥–∞–Ω—ñ\n",
    "        left_mask = X[:, best_feature] < best_thresh\n",
    "        right_mask = ~left_mask\n",
    "        random_subset_of_features1=self.random_feature(X.shape[1],2)\n",
    "        random_subset_of_features2=self.random_feature(X.shape[1],2)\n",
    "        left_subtree = self.build_tree(X[left_mask], y[left_mask], random_subset_of_features1,depth=depth+1)\n",
    "        right_subtree = self.build_tree(X[right_mask], y[right_mask], random_subset_of_features2,depth=depth+1)\n",
    "        return {\n",
    "            'is_leaf': False,\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_thresh,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "    def bootstrap_data(self):\n",
    "        indices = np.random.choice(len(self.x_data), len(self.x_data), replace=True)\n",
    "        return self.x_data[indices], self.y_data[indices]\n",
    "\n",
    "    #def launch_algorithm(self):\n",
    "    #    result=[]\n",
    "    #    for i in range(self.numb_of_tree):\n",
    "    #        featur_indice=self.random_feature(self.x_data.shape[1],2)\n",
    "    #        result1=self.build_tree(self.x_data,self.y_data,feature_indices=featur_indice)\n",
    "    #        result.append(result1)\n",
    "    #        print(\"hello\")\n",
    "    #    return result\n",
    "    def launch_algorithm(self):\n",
    "        result = []\n",
    "        for i in range(self.numb_of_tree):\n",
    "            X_sample, y_sample = self.bootstrap_data()\n",
    "            feature_indices = self.random_feature(self.x_data.shape[1], 2)\n",
    "            result1 = self.build_tree(X_sample, y_sample, feature_indices=feature_indices)\n",
    "            result.append(result1)\n",
    "            print(f\"Tree {i+1} –≥–æ—Ç–æ–≤–µ\")\n",
    "        return result\n",
    "    def predict_forest(self, x, forest):\n",
    "        predictions = []\n",
    "        for tree in forest:\n",
    "            pred = self.predict_tree(x, tree)\n",
    "            predictions.append(pred)\n",
    "        # –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è\n",
    "        values, counts = np.unique(predictions, return_counts=True)\n",
    "        majority_vote = values[np.argmax(counts)]\n",
    "        return majority_vote\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bce141-fd2e-4bfa-b2b6-c05b9696bd3d",
   "metadata": {},
   "source": [
    "## Classification Task 1: Inferring Sphere Density from Chord Distributions\n",
    "\n",
    "We aim to classify the density of spheres based on discretized chord length distributions obtained from horizontal cuts.\n",
    "\n",
    "- **Classes**: `distribution_11` (low density) vs. `distribution_17` (high density)\n",
    "- **Input**: a 10√ó1 feature vector (discretized chord lengths)\n",
    "- **Goal**: predict which class (density level) the feature vector belongs to\n",
    "- **Parameters**: \n",
    "  - `num_bins = 15` for finer discretization\n",
    "  - Use a simple classifier (e.g., Decision Tree) to distinguish between the two distributions\n",
    "\n",
    "This task mimics a supervised learning setting, where the model learns from labeled distributions and predicts the class of new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6370d06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11. 11. 11. ... 17. 17. 17.]\n"
     ]
    }
   ],
   "source": [
    "num_bins = 15\n",
    "num_cuts = 1000\n",
    "radius = 0.08\n",
    "\n",
    "positions = generate_spheres(11, radius)\n",
    "chord_lengths_set_11 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_11 = distribution_chord_lengths(chord_lengths_set_11, num_bins)\n",
    "\n",
    "positions = generate_spheres(17, radius)\n",
    "chord_lengths_set_17 = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution_17 = distribution_chord_lengths(chord_lengths_set_17, num_bins)\n",
    "\n",
    "X, Y = prepare_numpy_dataset([11, 17], radius, num_cuts, num_bins)\n",
    "\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0f777f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 –≥–æ—Ç–æ–≤–µ\n",
      "Tree 2 –≥–æ—Ç–æ–≤–µ\n",
      "Tree 3 –≥–æ—Ç–æ–≤–µ\n",
      "—Ç–æ—á–Ω—ñ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—É –¥–æ—Ä—ñ–≤–Ω—é—î = 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test=train_test_split(X=X,Y=Y)\n",
    "rand_forest=RandomForest(x_data=X_train,y_data=Y_train,numb_of_tree=3,depth=4)\n",
    "forest=rand_forest.launch_algorithm()\n",
    "\n",
    "def evaluate_accuracy(X_test, Y_test, forest, model):\n",
    "    correct = 0\n",
    "    for x, y_true in zip(X_test, Y_test):\n",
    "        y_pred = model.predict_forest(x, forest)\n",
    "        if y_pred == y_true:\n",
    "            correct += 1\n",
    "    return correct / len(Y_test)\n",
    "\n",
    "acc=evaluate_accuracy(X_test=X_test,Y_test=Y_test,forest=forest,model=rand_forest)\n",
    "print(f\"—Ç–æ—á–Ω—ñ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—É –¥–æ—Ä—ñ–≤–Ω—é—î = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fcf81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c232267",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(forest, 'my_forest.pkl')\n",
    "loaded_forest = load_model('my_forest.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4038baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#new_forest=RandomForest(x_data=[],y_data=[],numb_of_tree=3,depth=4)  \n",
    "#y_pred = new_forest.predict_forest(x, loaded_forest)      –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312148c-fc65-446a-9b6e-ffcda8f97f1b",
   "metadata": {},
   "source": [
    "## Regression Task 2: Inferring Sphere Radius from Chord Distributions\n",
    "\n",
    "We now approach a **regression** problem: estimating the true **radius** of the spheres based on the shape of their chord length distributions.\n",
    "\n",
    "- **Classes (Radii)**: choose two known radii, e.g. `r = 0.04` and `r = 0.08`\n",
    "- **Input**: a 10√ó1 discretized feature vector from chord lengths\n",
    "- **Output**: estimated radius of the sphere sample\n",
    "- **Goal**: train a regression model to learn the mapping from discretized distributions to true radius values\n",
    "\n",
    "This task demonstrates how statistical features derived from 2D cuts can be used to infer underlying 3D geometric properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c78b8dd-2250-4755-9157-8b34bafea5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–æ—Ä–º–∞ X: (166, 10)\n",
      "–§–æ—Ä–º–∞ Y: (166, 1)\n"
     ]
    }
   ],
   "source": [
    "######### for you to fill in\n",
    "import numpy as np\n",
    "\n",
    "# –§—É–Ω–∫—Ü—ñ—ó –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Å—Ñ–µ—Ä —ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ö–æ—Ä–¥ —Ç–∏ –≤–∂–µ –º–∞—î—à (generate_spheres, collect_chords_for_cuts, distribution_chord_lengths)\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
    "num_cuts = 100\n",
    "num_bins = 10\n",
    "\n",
    "# –î–≤–∞ —Ä—ñ–∑–Ω–∏—Ö —Ä–∞–¥—ñ—É—Å–∏\n",
    "radius_small = 0.04\n",
    "radius_large = 0.08\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–æ–∑–∏—Ü—ñ—ó —Å—Ñ–µ—Ä\n",
    "positions_small = generate_spheres(17, radius_small)\n",
    "positions_large = generate_spheres(17, radius_large)\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä—É—î–º–æ –º–Ω–æ–∂–∏–Ω–∏ —Ö–æ—Ä–¥\n",
    "chord_lengths_small = collect_chords_for_cuts(positions_small, radius_small, num_cuts)\n",
    "chord_lengths_large = collect_chords_for_cuts(positions_large, radius_large, num_cuts)\n",
    "\n",
    "# –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏ –¥–æ–≤–∂–∏–Ω —Ö–æ—Ä–¥\n",
    "distribution_small = distribution_chord_lengths(chord_lengths_small, num_bins)\n",
    "distribution_large = distribution_chord_lengths(chord_lengths_large, num_bins)\n",
    "\n",
    "# –§–æ—Ä–º—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç\n",
    "X_small = np.array(distribution_small)   # –†–æ–∑–ø–æ–¥—ñ–ª–∏ —è–∫ –æ–∑–Ω–∞–∫–∏\n",
    "X_large = np.array(distribution_large)\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —Ü—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (—Ä–∞–¥—ñ—É—Å–∏)\n",
    "Y_small = np.full((X_small.shape[0], 1), radius_small)\n",
    "Y_large = np.full((X_large.shape[0], 1), radius_large)\n",
    "\n",
    "# –û–±'—î–¥–Ω—É—î–º–æ –≤ —î–¥–∏–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "X = np.vstack((X_small, X_large))\n",
    "Y = np.vstack((Y_small, Y_large))\n",
    "\n",
    "# –í—Å–µ –≥–æ—Ç–æ–≤–æ\n",
    "print(\"–§–æ—Ä–º–∞ X:\", X.shape)\n",
    "print(\"–§–æ—Ä–º–∞ Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "15f86e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ (MSE): 0.00004\n",
      "–°–ø—Ä–∞–≤–∂–Ω—ñ —Ä–∞–¥—ñ—É—Å–∏: [0.08 0.08 0.08 0.04 0.08 0.04 0.08 0.08 0.08 0.08 0.04 0.04 0.04 0.04\n",
      " 0.04 0.04 0.08 0.04 0.08 0.04 0.08 0.04 0.04 0.04 0.08 0.08 0.08 0.04\n",
      " 0.04 0.08 0.04 0.08 0.08 0.08]\n",
      "–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ —Ä–∞–¥—ñ—É—Å–∏: [0.08       0.08       0.08       0.04333333 0.08       0.04333333\n",
      " 0.08       0.08       0.08       0.04333333 0.04333333 0.04333333\n",
      " 0.04333333 0.04333333 0.04333333 0.04333333 0.08       0.04333333\n",
      " 0.08       0.04333333 0.08       0.04333333 0.04333333 0.04333333\n",
      " 0.08       0.08       0.08       0.04333333 0.04333333 0.08\n",
      " 0.04333333 0.08       0.08       0.08      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —ñ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ —ñ –Ω–∞–≤—á–∞—î–º–æ –º–æ–¥–µ–ª—å –¥–µ—Ä–µ–≤–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó\n",
    "regressor = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "# –†–æ–±–∏–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "# –û—Ü—ñ–Ω—é—î–º–æ —è–∫—ñ—Å—Ç—å\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"–°–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ (MSE): {mse:.5f}\")\n",
    "\n",
    "# –ü—Ä–∏–∫–ª–∞–¥ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è\n",
    "print(\"–°–ø—Ä–∞–≤–∂–Ω—ñ —Ä–∞–¥—ñ—É—Å–∏:\", Y_test.flatten())\n",
    "print(\"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ —Ä–∞–¥—ñ—É—Å–∏:\", Y_pred.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67947d-8388-47ed-8433-d84aaea84b74",
   "metadata": {},
   "source": [
    "### üîß Lifehack: Sampling a Realistic Discretized Vector to save your time\n",
    "\n",
    "This small code snippet is a quick way to extract a **realistic discretized vector** from a generated chord length distribution. It simulates a sphere arrangement with known radius and density, collects chord lengths from horizontal cuts, and selects the **first meaningful vector** (i.e., one that contains actual data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079236ba-96fa-4a1a-8f2d-24ecd0b1b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample discretized vector: [0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_spheres = 17\n",
    "radius = 0.08\n",
    "num_cuts = 100\n",
    "num_bins = 10\n",
    "\n",
    "# Generate spheres and chord lengths\n",
    "positions = generate_spheres(num_spheres, radius)\n",
    "chord_lengths_set = collect_chords_for_cuts(positions, radius, num_cuts)\n",
    "distribution = distribution_chord_lengths(chord_lengths_set, num_bins)\n",
    "\n",
    "# Extract a single discretized vector (e.g., the first non-empty one)\n",
    "sample_vector = None\n",
    "for vec in distribution:\n",
    "    if vec.sum() > 0:  # Skip empty cuts\n",
    "        sample_vector = vec\n",
    "        break\n",
    "\n",
    "# Show the result\n",
    "print(\"Sample discretized vector:\", sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a37b22-ecf4-4abf-9c6b-cb1da795d260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
